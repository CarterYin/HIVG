# warmup stage
echo -e "\n\n\n\n\n\n\n==================== warmup stage  ==========================="
CUDA_VISIBLE_DEVICES=1,2,3,4,5,6,7 python -m torch.distributed.launch --nproc_per_node=7 --master_port 28887 --use_env hivg_train.py --num_workers 4 --epochs 60   --batch_size 64 --lr 0.00025   --lr_scheduler cosine --aug_crop --aug_scale --aug_translate  --vl_hidden_dim 512 --imsize 224 --max_query_len 77 --normalize_before --dataset mixup --use_contrastive_loss  --use_rtcc_constrain_loss --use_mask_loss  --mixup_pretrain --data_root /path_to_image_data --split_root /path_to_split/ref_data_shuffled  --clip_model /path_to_clip_checkpoint/clip_b_ml_cascade_maskrcnn_model_224.pth  --output_dir /path_to_output/output_v200/mixup --sup_type full;
CUDA_VISIBLE_DEVICES=1,2,3,4,5,6,7 python -m torch.distributed.launch --nproc_per_node=7 --master_port 28888 --use_env hivg_eval.py  --num_workers 2 --batch_size 64  --dataset mixup      --vl_hidden_dim 512 --imsize 224 --max_query_len 77 --normalize_before --use_mask_loss   --save_hilora_clip --mixup_pretrain --data_root /path_to_image_data --split_root /path_to_split/ref_data_shuffled --eval_model /path_to_output/output_v200/mixup/best_checkpoint.pth --eval_set val    --output_dir /path_to_output/output_v200/mixup;
CUDA_VISIBLE_DEVICES=1,2,3,4,5,6,7 python -m torch.distributed.launch --nproc_per_node=7 --master_port 28888 --use_env hivg_eval.py  --num_workers 2 --batch_size 64  --dataset mixup      --vl_hidden_dim 512 --imsize 224 --max_query_len 77 --normalize_before --use_mask_loss   --save_hilora_clip --mixup_pretrain --data_root /path_to_image_data --split_root /path_to_split/ref_data_shuffled --eval_model /path_to_output/output_v200/mixup/best_checkpoint.pth --eval_set test   --output_dir /path_to_output/output_v200/mixup;

# stage 1
echo -e "\n\n\n\n\n\n\n==================== mixup stage 1 ==========================="
CUDA_VISIBLE_DEVICES=1,2,3,4,5,6,7 python -m torch.distributed.launch --nproc_per_node=7 --master_port 28887 --use_env hivg_train.py --num_workers 4 --epochs 20   --batch_size 64 --lr 0.00010   --lr_scheduler cosine --aug_crop --aug_scale --aug_translate  --vl_hidden_dim 512 --imsize 224 --max_query_len 77 --normalize_before --dataset mixup    --use_contrastive_loss  --use_rtcc_constrain_loss --use_mask_loss  --mixup_pretrain   --hi_lora_stage 1 --data_root /path_to_image_data --split_root /path_to_split/ref_data_shuffled  --hi_lora_retrain /path_to_output/output_v200/mixup/checkpoint.pth --hi_lora_clip /path_to_output/output_v200/mixup/clip_lora_stage_with_bridge.pth  --output_dir /path_to_output/output_v201/mixup --sup_type full;
CUDA_VISIBLE_DEVICES=1,2,3,4,5,6,7 python -m torch.distributed.launch --nproc_per_node=7 --master_port 28888 --use_env hivg_eval.py  --num_workers 2 --batch_size 64  --dataset mixup       --vl_hidden_dim 512 --imsize 224 --max_query_len 77 --normalize_before --use_mask_loss   --save_hilora_clip --hi_lora_stage 1 --mixup_pretrain --data_root /path_to_image_data --split_root /path_to_split/ref_data_shuffled --eval_model /path_to_output/output_v201/mixup/best_checkpoint.pth --eval_set val    --output_dir /path_to_output/output_v201/mixup;
CUDA_VISIBLE_DEVICES=1,2,3,4,5,6,7 python -m torch.distributed.launch --nproc_per_node=7 --master_port 28888 --use_env hivg_eval.py  --num_workers 2 --batch_size 64  --dataset mixup       --vl_hidden_dim 512 --imsize 224 --max_query_len 77 --normalize_before --use_mask_loss   --save_hilora_clip --hi_lora_stage 1 --mixup_pretrain --data_root /path_to_image_data --split_root /path_to_split/ref_data_shuffled --eval_model /path_to_output/output_v201/mixup/best_checkpoint.pth --eval_set test   --output_dir /path_to_output/output_v201/mixup;

# stage 2
echo -e "\n\n\n\n\n\n\n==================== mixup stage 2 ==========================="
CUDA_VISIBLE_DEVICES=1,2,3,4,5,6,7 python -m torch.distributed.launch --nproc_per_node=7 --master_port 28887 --use_env hivg_train.py --num_workers 4 --epochs 20   --batch_size 64 --lr 0.000050  --lr_scheduler cosine --aug_crop --aug_scale --aug_translate  --vl_hidden_dim 512 --imsize 224 --max_query_len 77 --normalize_before --dataset mixup    --use_contrastive_loss  --use_rtcc_constrain_loss --use_mask_loss  --mixup_pretrain   --hi_lora_stage 2 --data_root /path_to_image_data --split_root /path_to_split/ref_data_shuffled  --hi_lora_retrain /path_to_output/output_v201/mixup/checkpoint.pth --hi_lora_clip /path_to_output/output_v201/mixup/clip_lora_stage_with_bridge.pth  --output_dir /path_to_output/output_v202/mixup --sup_type full;
CUDA_VISIBLE_DEVICES=1,2,3,4,5,6,7 python -m torch.distributed.launch --nproc_per_node=7 --master_port 28888 --use_env hivg_eval.py  --num_workers 2 --batch_size 64  --dataset mixup       --vl_hidden_dim 512 --imsize 224 --max_query_len 77 --normalize_before --use_mask_loss   --save_hilora_clip --hi_lora_stage 2 --mixup_pretrain --data_root /path_to_image_data --split_root /path_to_split/ref_data_shuffled --eval_model /path_to_output/output_v202/mixup/best_checkpoint.pth --eval_set val    --output_dir /path_to_output/output_v202/mixup;
CUDA_VISIBLE_DEVICES=1,2,3,4,5,6,7 python -m torch.distributed.launch --nproc_per_node=7 --master_port 28888 --use_env hivg_eval.py  --num_workers 2 --batch_size 64  --dataset mixup       --vl_hidden_dim 512 --imsize 224 --max_query_len 77 --normalize_before --use_mask_loss   --save_hilora_clip --hi_lora_stage 2 --mixup_pretrain --data_root /path_to_image_data --split_root /path_to_split/ref_data_shuffled --eval_model /path_to_output/output_v202/mixup/best_checkpoint.pth --eval_set test   --output_dir /path_to_output/output_v202/mixup;

# stage 3
echo -e "\n\n\n\n\n\n\n==================== mixup stage 3 ==========================="
CUDA_VISIBLE_DEVICES=1,2,3,4,5,6,7 python -m torch.distributed.launch --nproc_per_node=7 --master_port 28887 --use_env hivg_train.py --num_workers 4 --epochs 20   --batch_size 64 --lr 0.000020  --lr_scheduler cosine --aug_crop --aug_scale --aug_translate  --vl_hidden_dim 512 --imsize 224 --max_query_len 77 --normalize_before --dataset mixup    --use_contrastive_loss  --use_rtcc_constrain_loss --use_mask_loss  --mixup_pretrain   --hi_lora_stage 3 --data_root /path_to_image_data --split_root /path_to_split/ref_data_shuffled  --hi_lora_retrain /path_to_output/output_v202/mixup/checkpoint.pth --hi_lora_clip /path_to_output/output_v202/mixup/clip_lora_stage_with_bridge.pth  --output_dir /path_to_output/output_v203/mixup --sup_type full;
CUDA_VISIBLE_DEVICES=1,2,3,4,5,6,7 python -m torch.distributed.launch --nproc_per_node=7 --master_port 28888 --use_env hivg_eval.py  --num_workers 2 --batch_size 64  --dataset mixup       --vl_hidden_dim 512 --imsize 224 --max_query_len 77 --normalize_before --use_mask_loss   --save_hilora_clip --hi_lora_stage 3 --mixup_pretrain --data_root /path_to_image_data --split_root /path_to_split/ref_data_shuffled --eval_model /path_to_output/output_v203/mixup/best_checkpoint.pth --eval_set val    --output_dir /path_to_output/output_v203/mixup;
CUDA_VISIBLE_DEVICES=1,2,3,4,5,6,7 python -m torch.distributed.launch --nproc_per_node=7 --master_port 28888 --use_env hivg_eval.py  --num_workers 2 --batch_size 64  --dataset mixup       --vl_hidden_dim 512 --imsize 224 --max_query_len 77 --normalize_before --use_mask_loss   --save_hilora_clip --hi_lora_stage 3 --mixup_pretrain --data_root /path_to_image_data --split_root /path_to_split/ref_data_shuffled --eval_model /path_to_output/output_v203/mixup/best_checkpoint.pth --eval_set test   --output_dir /path_to_output/output_v203/mixup;
##

